{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba3f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2548d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b839834",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed12ea",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da8558ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('./t5-base/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56cf3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 获取字典\n",
    "# CV_topic_dic = {}\n",
    "# with open(\"./LectureBank/CV/CV.topics.tsv\",'r',encoding='utf-8') as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines:\n",
    "#         line = line.strip().split(\"\\t\")\n",
    "#         CV_topic_dic[line[0]] = line[1]\n",
    "# CV_topic_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28175fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 获取字典\n",
    "# BIO_topic_dic = {}\n",
    "# with open(\"./LectureBank/BIO/BIO.topics.tsv\",'r',encoding='utf-8') as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines:\n",
    "#         line = line.strip().split(\"\\t\")\n",
    "#         BIO_topic_dic[line[0]] = line[1]\n",
    "# BIO_topic_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b357422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./wiki80/wiki80_train.txt\",'r',encoding='utf-8') as f:\n",
    "#         lines = f.readlines()\n",
    "#         for line in lines:\n",
    "#             data = json.loads(line)\n",
    "#             h_entity_replace = randomLetter()\n",
    "#             t_entity_replace = randomLetter()\n",
    "#             plus_text = data['token'][0:data['h']['pos'][0]] + [h_entity_replace] +  data['token'][data['h']['pos'][1]:data['t']['pos'][0]] + [t_entity_replace] + data['token'][data['h']['pos'][1]:] \n",
    "#             plus_text = \" \".join(plus_text)\n",
    "#             plus_text = \"extract relation: \" + plus_text.lower() + \" </s>\"\n",
    "#             rel_text = h_entity_replace.lower() + \" - \" +  data['relation'].lower() + ' - ' + t_entity_replace.lower() + \" </s>\"\n",
    "#             print(plus_text)\n",
    "#             print(rel_text)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffb0091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filepath,tokenizer):\n",
    "    origin_texts = []\n",
    "    rel_texts = []\n",
    "    max_input_len = 0\n",
    "    max_output_len = 0\n",
    "    with open(filepath,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip().split(\",\")\n",
    "            lecture1 = line[0].replace(\"_\",\" \")\n",
    "            lecture2 = line[1].replace(\"_\",\" \")\n",
    "            text = \"judge prerequisite: \" + lecture1 + \" \" + lecture2 + \" </s>\"\n",
    "            if line[2] == \"0\":\n",
    "                rel = \"is not prerequisite\"\n",
    "            elif line[2] == \"1\":\n",
    "                rel = \"is prerequisite\"\n",
    "            rel_text = lecture1 + \" - \" +  rel + ' - ' + lecture2 + \" </s>\"\n",
    "            origin_texts.append(text)\n",
    "            rel_texts.append(rel_text)\n",
    "            tokenized_inp = tokenizer.encode_plus(text, return_tensors=\"pt\")\n",
    "            tokenized_output = tokenizer.encode_plus(rel_text,return_tensors=\"pt\")\n",
    "            input_ids  = tokenized_inp[\"input_ids\"]\n",
    "            output_ids  = tokenized_output[\"input_ids\"]\n",
    "            max_input_len = max(max_input_len, input_ids.shape[1])\n",
    "            max_output_len = max(max_output_len, output_ids.shape[1])\n",
    "            \n",
    "            # 数据增强\n",
    "            # 用两个随机字母替换两个实体\n",
    "#             h_entity_replace = randomLetter()\n",
    "#             t_entity_replace = randomLetter()\n",
    "#             first_entity_pos = []\n",
    "#             second_entity_pos = []\n",
    "#             if data['h']['pos'][0] < data['t']['pos'][0]:\n",
    "#                 first_entity_pos = data['h']['pos']\n",
    "#                 second_entity_pos = data['t']['pos']\n",
    "#             else:\n",
    "#                 first_entity_pos = data['t']['pos']\n",
    "#                 second_entity_pos = data['h']['pos']\n",
    "#             plus_text = data['token'][0:first_entity_pos[0]] + [h_entity_replace] +  data['token'][first_entity_pos[1]:second_entity_pos[0]] + [t_entity_replace] + data['token'][second_entity_pos[1]:] \n",
    "#             plus_text = \" \".join(plus_text)\n",
    "#             plus_text = \"extract relation: \" + plus_text.lower() + \" </s>\"\n",
    "#             rel_text = h_entity_replace.lower() + \" - \" +  data['relation'].lower() + ' - ' + t_entity_replace.lower() + \" </s>\"\n",
    "#             print(plus_text)\n",
    "#             print(rel_text)\n",
    "#             origin_texts.append(plus_text)\n",
    "#             rel_texts.append(rel_text)\n",
    "    print(max_input_len)\n",
    "    print(max_output_len)\n",
    "    return origin_texts,rel_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f01d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_val_data(filepath,tokenizer):\n",
    "    origin_texts = []\n",
    "    rel_texts = []\n",
    "    max_input_len = 0\n",
    "    max_output_len = 0\n",
    "    with open(filepath,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip().split(\",\")\n",
    "            lecture1 = line[0].replace(\"_\",\" \")\n",
    "            lecture2 = line[1].replace(\"_\",\" \")\n",
    "            text = \"judge prerequisite: \" + lecture1 + \" \" + lecture2 + \" </s>\"\n",
    "            if line[2] == \"0\":\n",
    "                rel = \"is not prerequisite\"\n",
    "            elif line[2] == \"1\":\n",
    "                rel = \"is prerequisite\"\n",
    "            rel_text = lecture1 + \" - \" +  rel + ' - ' + lecture2\n",
    "            origin_texts.append(text)\n",
    "            rel_texts.append(rel_text)\n",
    "    return origin_texts,rel_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "223159f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extract_Dataset(Dataset):\n",
    "    def __init__(self, filepaths, tokenizer,max_input_len,max_output_len):\n",
    "        self.origin_texts, self.rel_texts = [],[]\n",
    "        for filepath in filepaths:\n",
    "            o,r = extract_data(filepath,tokenizer)\n",
    "            self.origin_texts.extend(o)\n",
    "            self.rel_texts.extend(r)\n",
    "        self.max_input_len = max_input_len\n",
    "        self.max_output_len = max_output_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.origin_texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        tokenized_input = tokenizer.encode_plus(self.origin_texts[index], max_length=self.max_input_len, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "        tokenized_output = tokenizer.encode_plus(self.rel_texts[index], max_length=self.max_output_len, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "        \n",
    "        input_ids  = tokenized_input[\"input_ids\"].squeeze()\n",
    "        attention_mask = tokenized_input[\"attention_mask\"].squeeze()\n",
    "\n",
    "        output_ids = tokenized_output[\"input_ids\"].squeeze()\n",
    "        decoder_attention_mask=  tokenized_output[\"attention_mask\"].squeeze()\n",
    "        \n",
    "        data = {\n",
    "            'input_ids':input_ids,\n",
    "            'attention_mask':attention_mask,\n",
    "            'output_ids':output_ids,\n",
    "            'decoder_attention_mask':decoder_attention_mask\n",
    "        }\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e14bdc",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f294718",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('./t5-base/').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "359c3a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in t5_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in t5_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d25e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer):   # 训练模型\n",
    "    model.train()\n",
    "    with tqdm(total=len(train_loader)) as bar:\n",
    "        for idx,data in enumerate(train_loader):\n",
    "            input_ids, attention_mask, output_ids, decoder_attention_mask = data['input_ids'].to(device), data['attention_mask'].to(device), data['output_ids'].to(device), data['decoder_attention_mask'].to(device)\n",
    "            output = model(input_ids=input_ids, labels=output_ids,decoder_attention_mask=decoder_attention_mask,attention_mask=attention_mask)\n",
    "            loss = output[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            bar.set_postfix(loss=loss.item())\n",
    "            bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1e7d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, tokenizer,device, filepaths):\n",
    "    origin_texts, rel_texts = [], []\n",
    "    for filepath in filepaths:\n",
    "        o,r= extract_val_data(filepath,tokenizer)\n",
    "        origin_texts.extend(o)\n",
    "        rel_texts.extend(r)\n",
    "    model.eval()\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    with tqdm(total=len(origin_texts)) as bar:\n",
    "        for idx, text in enumerate(origin_texts):\n",
    "            input_ids = tokenizer(text, return_tensors='pt').input_ids.to(DEVICE)\n",
    "            outputs = t5_model.generate(input_ids)\n",
    "            result = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\" - \")\n",
    "            rel_text = rel_texts[idx].split(\" - \")\n",
    "#             print(rel_text)\n",
    "#             print(result)\n",
    "            if len(result) > 1 and result[1] == \"is prerequisite\":\n",
    "#                 print(1)\n",
    "                if result[1] == rel_text[1]:\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    FP += 1\n",
    "            else:\n",
    "                if result[1] != rel_text[1]:\n",
    "                    FN += 1\n",
    "            bar.update(1)\n",
    "    print(TP)\n",
    "    print(FP)\n",
    "    print(FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision,recall,F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef3dfebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def val(model, tokenizer,device, filepath):\n",
    "#     origin_texts, rel_texts = extract_val_data(filepath,tokenizer)\n",
    "#     model.eval()\n",
    "#     rel_acc_num = 0\n",
    "#     total = 0\n",
    "#     all_acc_num = 0\n",
    "#     entity_acc_num = 0\n",
    "#     with tqdm(total=len(origin_texts)) as bar:\n",
    "#         for idx, text in enumerate(origin_texts):\n",
    "#             total += 1\n",
    "#             input_ids = tokenizer(text, return_tensors='pt').input_ids.to(DEVICE)\n",
    "#             outputs = t5_model.generate(input_ids)\n",
    "#             result = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\" - \")\n",
    "#             rel_text = rel_texts[idx].split(\" - \")\n",
    "# #             print(rel_text)\n",
    "# #             print(result)\n",
    "#             if len(result) > 1 and rel_text[1] == result[1]:\n",
    "#                 rel_acc_num += 1\n",
    "#             if len(result) > 2 and rel_text[0] == result[0] and rel_text[2] == result[2]:\n",
    "#                 entity_acc_num += 1\n",
    "#             if len(result) > 2 and rel_text[1] == result[1] and rel_text[0] == result[0] and rel_text[2] == result[2]:\n",
    "#                 all_acc_num += 1\n",
    "#             bar.update(1)\n",
    "#     return str(rel_acc_num / total),str(entity_acc_num / total),str(all_acc_num / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37558edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyq/miniconda3/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:174: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "28\n",
      "24\n",
      "28\n",
      "24\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "#filepaths = [\"./UniversityCourse/train_0.txt\",\"./UniversityCourse/train_1.txt\",\"./UniversityCourse/train_2.txt\",\"./UniversityCourse/train_3.txt\",\"./UniversityCourse/train_4.txt\"]\n",
    "filepaths = [\"./UniversityCourse/train_0.txt\",\"./UniversityCourse/train_1.txt\",\"./UniversityCourse/train_2.txt\"]\n",
    "train_dataset = Extract_Dataset(filepaths=filepaths,tokenizer=tokenizer,max_input_len=40,max_output_len=40)\n",
    "train_loader = DataLoader(train_dataset,shuffle=True,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "705f04f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/171 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/zyq/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 171/171 [00:26<00:00,  6.46it/s, loss=0.0241]\n",
      "100%|██████████| 2255/2255 [06:55<00:00,  5.42it/s]\n",
      "  1%|          | 1/171 [00:00<00:26,  6.32it/s, loss=0.0169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "788\n",
      "454\n",
      "217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:26<00:00,  6.57it/s, loss=0.00362]\n",
      "100%|██████████| 2255/2255 [06:53<00:00,  5.45it/s]\n",
      "  1%|          | 1/171 [00:00<00:26,  6.37it/s, loss=0.0149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n",
      "569\n",
      "62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:25<00:00,  6.59it/s, loss=0.0179] \n",
      "100%|██████████| 2255/2255 [06:48<00:00,  5.51it/s]\n",
      "  1%|          | 1/171 [00:00<00:27,  6.28it/s, loss=0.0123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976\n",
      "543\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:26<00:00,  6.56it/s, loss=0.00932]\n",
      "100%|██████████| 2255/2255 [06:54<00:00,  5.43it/s]\n",
      "  1%|          | 1/171 [00:00<00:27,  6.28it/s, loss=0.00577]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986\n",
      "470\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:25<00:00,  6.60it/s, loss=0.0089] \n",
      "100%|██████████| 2255/2255 [07:00<00:00,  5.36it/s]\n",
      "  1%|          | 1/171 [00:00<00:26,  6.37it/s, loss=0.00524]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "923\n",
      "206\n",
      "82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:26<00:00,  6.54it/s, loss=0.00129]\n",
      "100%|██████████| 2255/2255 [06:53<00:00,  5.46it/s]\n",
      "  1%|          | 1/171 [00:00<00:26,  6.38it/s, loss=0.00403]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983\n",
      "269\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:26<00:00,  6.56it/s, loss=0.00024]\n",
      "100%|██████████| 2255/2255 [06:53<00:00,  5.45it/s]\n",
      "  1%|          | 1/171 [00:00<00:26,  6.34it/s, loss=0.00591]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "335\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:25<00:00,  6.58it/s, loss=0.00051]\n",
      "100%|██████████| 2255/2255 [06:57<00:00,  5.40it/s]\n",
      "  1%|          | 1/171 [00:00<00:26,  6.35it/s, loss=0.00111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002\n",
      "304\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:25<00:00,  6.59it/s, loss=0.000212]\n",
      "100%|██████████| 2255/2255 [06:53<00:00,  5.45it/s]\n",
      "  1%|          | 1/171 [00:00<00:26,  6.31it/s, loss=0.00165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002\n",
      "227\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:26<00:00,  6.57it/s, loss=2.98e-5] \n",
      "100%|██████████| 2255/2255 [06:54<00:00,  5.44it/s]\n",
      "  1%|          | 1/171 [00:00<00:26,  6.36it/s, loss=0.00609]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003\n",
      "258\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:25<00:00,  6.60it/s, loss=7.62e-5] \n",
      "100%|██████████| 2255/2255 [06:53<00:00,  5.46it/s]\n",
      "  1%|          | 1/171 [00:00<00:26,  6.44it/s, loss=0.000674]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005\n",
      "237\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:25<00:00,  6.60it/s, loss=0.000156]\n",
      "100%|██████████| 2255/2255 [06:57<00:00,  5.41it/s]\n",
      "  1%|          | 1/171 [00:00<00:26,  6.34it/s, loss=0.000961]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n",
      "211\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:25<00:00,  6.60it/s, loss=5.05e-5] \n",
      "100%|██████████| 2255/2255 [06:54<00:00,  5.45it/s]\n",
      "  1%|          | 1/171 [00:00<00:26,  6.38it/s, loss=0.000551]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002\n",
      "189\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:26<00:00,  6.57it/s, loss=0.000456]\n",
      "100%|██████████| 2255/2255 [06:55<00:00,  5.43it/s]\n",
      "  1%|          | 1/171 [00:00<00:27,  6.27it/s, loss=0.000207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004\n",
      "230\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:25<00:00,  6.58it/s, loss=0.0823]  \n",
      "100%|██████████| 2255/2255 [06:52<00:00,  5.47it/s]\n",
      "  1%|          | 1/171 [00:00<00:26,  6.38it/s, loss=0.00169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "164\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:25<00:00,  6.59it/s, loss=0.000198]\n",
      "100%|██████████| 2255/2255 [06:53<00:00,  5.46it/s]\n",
      "  1%|          | 1/171 [00:00<00:26,  6.33it/s, loss=0.000133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004\n",
      "202\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:25<00:00,  6.60it/s, loss=0.000139]\n",
      "100%|██████████| 2255/2255 [06:52<00:00,  5.46it/s]\n",
      "  1%|          | 1/171 [00:00<00:26,  6.34it/s, loss=0.000118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002\n",
      "166\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:26<00:00,  6.57it/s, loss=0.00243] \n",
      "100%|██████████| 2255/2255 [06:55<00:00,  5.43it/s]\n",
      "  1%|          | 1/171 [00:00<00:26,  6.39it/s, loss=0.00125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "155\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:25<00:00,  6.70it/s, loss=0.00641] \n",
      "100%|██████████| 2255/2255 [06:54<00:00,  5.44it/s]\n",
      "  1%|          | 1/171 [00:00<00:26,  6.37it/s, loss=0.00039]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003\n",
      "192\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:25<00:00,  6.58it/s, loss=0.000125]\n",
      "100%|██████████| 2255/2255 [06:54<00:00,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004\n",
      "277\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHES = 20\n",
    "with open(\"./log(universityCourse60%).txt\",'w',encoding='utf-8') as f:\n",
    "    for epoch in range(NUM_EPOCHES):\n",
    "        train(t5_model,DEVICE,train_loader,optimizer)\n",
    "        precision,recall,F1 = val(t5_model,tokenizer,DEVICE,[\"./UniversityCourse/test_0.txt\",\"./UniversityCourse/test_1.txt\",\"./UniversityCourse/test_2.txt\",\"./UniversityCourse/test_3.txt\",\"./UniversityCourse/test_4.txt\"])\n",
    "        f.write(\"EPOCH:\" + str(epoch) + \"\\n\")\n",
    "        f.write(\"precision：\" + str(precision) + \"\\n\")\n",
    "        f.write(\"recall：\" + str(recall) + \"\\n\")\n",
    "        f.write(\"F1：\" + str(F1) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "070fa217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t =  \" \".join([\"Flight\", \"3400\", \"was\", \"bound\", \"for\", \"Moline\", \",\", \"Il\", \".\", \",\", \"when\", \"it\", \"was\", \"diverted\", \"about\", \"9\", \"p.m.\", \"to\", \"Majors\", \"Airport\", \"in\", \"Greenville\", \".\"])\n",
    "# t = \"extract relation: \" + t.lower() + \" </s>\"\n",
    "# print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a21de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = tokenizer(t, return_tensors='pt').input_ids.to(DEVICE)\n",
    "# outputs = t5_model.generate(input_ids)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4d900a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# result.split(\" - \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85d62bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(val(t5_model,tokenizer,DEVICE,\"./semeval/semeval_val.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86acf595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_rel,acc_entity,acc_all = val(t5_model,tokenizer,DEVICE,\"./LectureBank/val.0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95ec6fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7e2d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488cd59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
