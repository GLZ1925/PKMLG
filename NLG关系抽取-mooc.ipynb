{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba3f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2548d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b839834",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed12ea",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da8558ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('./t5-base/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b357422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./wiki80/wiki80_train.txt\",'r',encoding='utf-8') as f:\n",
    "#         lines = f.readlines()\n",
    "#         for line in lines:\n",
    "#             data = json.loads(line)\n",
    "#             h_entity_replace = randomLetter()\n",
    "#             t_entity_replace = randomLetter()\n",
    "#             plus_text = data['token'][0:data['h']['pos'][0]] + [h_entity_replace] +  data['token'][data['h']['pos'][1]:data['t']['pos'][0]] + [t_entity_replace] + data['token'][data['h']['pos'][1]:] \n",
    "#             plus_text = \" \".join(plus_text)\n",
    "#             plus_text = \"extract relation: \" + plus_text.lower() + \" </s>\"\n",
    "#             rel_text = h_entity_replace.lower() + \" - \" +  data['relation'].lower() + ' - ' + t_entity_replace.lower() + \" </s>\"\n",
    "#             print(plus_text)\n",
    "#             print(rel_text)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb0091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filepath,tokenizer):\n",
    "    origin_texts = []\n",
    "    rel_texts = []\n",
    "    max_input_len = 0\n",
    "    max_output_len = 0\n",
    "    not_pre = 0\n",
    "    pre = 0\n",
    "    with open(filepath,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            lecture1 = line[0]\n",
    "            lecture2 = line[2]\n",
    "            text = \"judge prerequisite: \" + lecture1 + \" \" + lecture2 + \" </s>\"\n",
    "            if line[4] == \"-\":\n",
    "                rel = \"is not prerequisite\"\n",
    "                not_pre += 1\n",
    "            elif line[4] == \"1-\":\n",
    "                rel = \"is prerequisite\"\n",
    "                pre +=1\n",
    "            elif line[4] == \"-1\":\n",
    "                rel = \"is prerequisite\"\n",
    "                lecture1,lecture2 = lecture2,lecture1\n",
    "                pre +=1\n",
    "            else:\n",
    "                print(line)\n",
    "                print(\"!\")\n",
    "            rel_text = lecture1 + \" - \" +  rel + ' - ' + lecture2 + \" </s>\"\n",
    "            origin_texts.append(text)\n",
    "            rel_texts.append(rel_text)\n",
    "            tokenized_inp = tokenizer.encode_plus(text, return_tensors=\"pt\")\n",
    "            tokenized_output = tokenizer.encode_plus(rel_text,return_tensors=\"pt\")\n",
    "            input_ids  = tokenized_inp[\"input_ids\"]\n",
    "            output_ids  = tokenized_output[\"input_ids\"]\n",
    "            max_input_len = max(max_input_len, input_ids.shape[1])\n",
    "            max_output_len = max(max_output_len, output_ids.shape[1])\n",
    "            \n",
    "            # 数据增强\n",
    "            # 用两个随机字母替换两个实体\n",
    "#             h_entity_replace = randomLetter()\n",
    "#             t_entity_replace = randomLetter()\n",
    "#             first_entity_pos = []\n",
    "#             second_entity_pos = []\n",
    "#             if data['h']['pos'][0] < data['t']['pos'][0]:\n",
    "#                 first_entity_pos = data['h']['pos']\n",
    "#                 second_entity_pos = data['t']['pos']\n",
    "#             else:\n",
    "#                 first_entity_pos = data['t']['pos']\n",
    "#                 second_entity_pos = data['h']['pos']\n",
    "#             plus_text = data['token'][0:first_entity_pos[0]] + [h_entity_replace] +  data['token'][first_entity_pos[1]:second_entity_pos[0]] + [t_entity_replace] + data['token'][second_entity_pos[1]:] \n",
    "#             plus_text = \" \".join(plus_text)\n",
    "#             plus_text = \"extract relation: \" + plus_text.lower() + \" </s>\"\n",
    "#             rel_text = h_entity_replace.lower() + \" - \" +  data['relation'].lower() + ' - ' + t_entity_replace.lower() + \" </s>\"\n",
    "#             print(plus_text)\n",
    "#             print(rel_text)\n",
    "#             origin_texts.append(plus_text)\n",
    "#             rel_texts.append(rel_text)\n",
    "#     print(max_input_len)\n",
    "#     print(max_output_len)\n",
    "    print(pre)\n",
    "    print(not_pre)\n",
    "    return origin_texts,rel_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f01d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_val_data(filepath,tokenizer):\n",
    "    origin_texts = []\n",
    "    rel_texts = []\n",
    "    max_input_len = 0\n",
    "    max_output_len = 0\n",
    "    with open(filepath,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        rel = \"\"\n",
    "        for line in lines:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            lecture1 = line[0]\n",
    "            lecture2 = line[2]\n",
    "            text = \"judge prerequisite: \" + lecture1 + \" \" + lecture2 + \" </s>\"\n",
    "            if line[4] == \"-\":\n",
    "                rel = \"is not prerequisite\"\n",
    "            elif line[4] == \"-1\" or line[4] == \"1-\":\n",
    "                rel = \"is prerequisite\"\n",
    "            else:\n",
    "                print(line)\n",
    "                print(\"!\")\n",
    "            rel_text = lecture1 + \" - \" +  rel + ' - ' + lecture2\n",
    "            origin_texts.append(text)\n",
    "            rel_texts.append(rel_text)\n",
    "    return origin_texts,rel_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "223159f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extract_Dataset(Dataset):\n",
    "    def __init__(self, filepaths, tokenizer,max_input_len,max_output_len):\n",
    "        self.origin_texts, self.rel_texts = [],[]\n",
    "        for filepath in filepaths:\n",
    "            o,r = extract_data(filepath,tokenizer)\n",
    "            self.origin_texts.extend(o)\n",
    "            self.rel_texts.extend(r)\n",
    "        self.max_input_len = max_input_len\n",
    "        self.max_output_len = max_output_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.origin_texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        tokenized_input = tokenizer.encode_plus(self.origin_texts[index], max_length=self.max_input_len, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "        tokenized_output = tokenizer.encode_plus(self.rel_texts[index], max_length=self.max_output_len, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "        \n",
    "        input_ids  = tokenized_input[\"input_ids\"].squeeze()\n",
    "        attention_mask = tokenized_input[\"attention_mask\"].squeeze()\n",
    "\n",
    "        output_ids = tokenized_output[\"input_ids\"].squeeze()\n",
    "        decoder_attention_mask=  tokenized_output[\"attention_mask\"].squeeze()\n",
    "        \n",
    "        data = {\n",
    "            'input_ids':input_ids,\n",
    "            'attention_mask':attention_mask,\n",
    "            'output_ids':output_ids,\n",
    "            'decoder_attention_mask':decoder_attention_mask\n",
    "        }\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e14bdc",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f294718",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('./t5-base/').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "359c3a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in t5_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in t5_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d25e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer):   # 训练模型\n",
    "    model.train()\n",
    "    with tqdm(total=len(train_loader)) as bar:\n",
    "        for idx,data in enumerate(train_loader):\n",
    "            input_ids, attention_mask, output_ids, decoder_attention_mask = data['input_ids'].to(device), data['attention_mask'].to(device), data['output_ids'].to(device), data['decoder_attention_mask'].to(device)\n",
    "            output = model(input_ids=input_ids, labels=output_ids,decoder_attention_mask=decoder_attention_mask,attention_mask=attention_mask)\n",
    "            loss = output[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            bar.set_postfix(loss=loss.item())\n",
    "            bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef3dfebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, tokenizer,device, filepath):\n",
    "    origin_texts, rel_texts = extract_val_data(filepath,tokenizer)\n",
    "    model.eval()\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    with tqdm(total=len(origin_texts)) as bar:\n",
    "        for idx, text in enumerate(origin_texts):\n",
    "            input_ids = tokenizer(text, return_tensors='pt').input_ids.to(DEVICE)\n",
    "            outputs = t5_model.generate(input_ids)\n",
    "            result = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\" - \")\n",
    "            rel_text = rel_texts[idx].split(\" - \")\n",
    "#             print(rel_text)\n",
    "#             print(result)\n",
    "            if result[1] == \"is prerequisite\":\n",
    "                # print(1)\n",
    "                if result[1] == rel_text[1]:\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    FP += 1\n",
    "            else:\n",
    "                if result[1] != rel_text[1]:\n",
    "                    FN += 1\n",
    "            bar.update(1)\n",
    "    print(TP)\n",
    "    print(FP)\n",
    "    print(FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision,recall,F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37558edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyq/miniconda3/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:174: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1735\n",
      "4977\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\"./Mooc/ML/ML_LabeledFile\"]\n",
    "train_dataset = Extract_Dataset(filepaths=filepaths,tokenizer=tokenizer,max_input_len=40,max_output_len=40)\n",
    "train_loader = DataLoader(train_dataset,shuffle=True,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "705f04f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/210 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/zyq/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 210/210 [00:32<00:00,  6.48it/s, loss=0.0203]\n",
      "100%|██████████| 935/935 [02:42<00:00,  5.77it/s]\n",
      "  0%|          | 1/210 [00:00<00:33,  6.32it/s, loss=0.0163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "29\n",
      "216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:32<00:00,  6.56it/s, loss=0.0242] \n",
      "100%|██████████| 935/935 [02:40<00:00,  5.82it/s]\n",
      "  0%|          | 1/210 [00:00<00:33,  6.29it/s, loss=0.0171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "36\n",
      "197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:32<00:00,  6.54it/s, loss=0.0168] \n",
      "100%|██████████| 935/935 [02:40<00:00,  5.82it/s]\n",
      "  0%|          | 1/210 [00:00<00:32,  6.36it/s, loss=0.0145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "65\n",
      "108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:31<00:00,  6.57it/s, loss=0.0145] \n",
      "100%|██████████| 935/935 [02:41<00:00,  5.80it/s]\n",
      "  0%|          | 1/210 [00:00<00:32,  6.34it/s, loss=0.0119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n",
      "49\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:31<00:00,  6.57it/s, loss=0.0105] \n",
      "100%|██████████| 935/935 [02:40<00:00,  5.82it/s]\n",
      "  0%|          | 1/210 [00:00<00:33,  6.25it/s, loss=0.01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n",
      "35\n",
      "113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:32<00:00,  6.51it/s, loss=0.00967]\n",
      "100%|██████████| 935/935 [02:39<00:00,  5.85it/s]\n",
      "  0%|          | 1/210 [00:00<00:32,  6.36it/s, loss=0.00821]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "28\n",
      "72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:31<00:00,  6.57it/s, loss=0.0124] \n",
      "100%|██████████| 935/935 [02:39<00:00,  5.87it/s]\n",
      "  0%|          | 1/210 [00:00<00:32,  6.37it/s, loss=0.0089]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n",
      "18\n",
      "71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:32<00:00,  6.56it/s, loss=0.00415]\n",
      "100%|██████████| 935/935 [02:39<00:00,  5.85it/s]\n",
      "  0%|          | 1/210 [00:00<00:33,  6.32it/s, loss=0.00505]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n",
      "27\n",
      "59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:32<00:00,  6.55it/s, loss=0.0211] \n",
      "100%|██████████| 935/935 [02:38<00:00,  5.89it/s]\n",
      "  0%|          | 1/210 [00:00<00:33,  6.31it/s, loss=0.0112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n",
      "56\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:31<00:00,  6.57it/s, loss=0.00227]\n",
      "100%|██████████| 935/935 [02:40<00:00,  5.84it/s]\n",
      "  0%|          | 1/210 [00:00<00:31,  6.56it/s, loss=0.00172]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n",
      "36\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:32<00:00,  6.54it/s, loss=0.0021] \n",
      "100%|██████████| 935/935 [02:39<00:00,  5.88it/s]\n",
      "  0%|          | 1/210 [00:00<00:33,  6.31it/s, loss=0.00269]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "18\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:32<00:00,  6.53it/s, loss=0.00475] \n",
      " 57%|█████▋    | 535/935 [01:30<00:59,  6.73it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHES = 20\n",
    "with open(\"./log(mooc).txt\",'w',encoding='utf-8') as f:\n",
    "    for epoch in range(NUM_EPOCHES):\n",
    "        train(t5_model,DEVICE,train_loader,optimizer)\n",
    "        precision,recall,F1 = val(t5_model,tokenizer,DEVICE,\"./Mooc/ML/W-ML_LabeledFile\")\n",
    "        f.write(\"EPOCH:\" + str(epoch) + \"\\n\")\n",
    "        f.write(\"precision：\" + str(precision) + \"\\n\")\n",
    "        f.write(\"recall：\" + str(recall) + \"\\n\")\n",
    "        f.write(\"F1：\" + str(F1) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ad7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755dc9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074452fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "070fa217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t =  \" \".join([\"Flight\", \"3400\", \"was\", \"bound\", \"for\", \"Moline\", \",\", \"Il\", \".\", \",\", \"when\", \"it\", \"was\", \"diverted\", \"about\", \"9\", \"p.m.\", \"to\", \"Majors\", \"Airport\", \"in\", \"Greenville\", \".\"])\n",
    "# t = \"extract relation: \" + t.lower() + \" </s>\"\n",
    "# print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a21de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = tokenizer(t, return_tensors='pt').input_ids.to(DEVICE)\n",
    "# outputs = t5_model.generate(input_ids)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4d900a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# result.split(\" - \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85d62bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(val(t5_model,tokenizer,DEVICE,\"./semeval/semeval_val.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86acf595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_rel,acc_entity,acc_all = val(t5_model,tokenizer,DEVICE,\"./LectureBank/val.0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95ec6fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7e2d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488cd59b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
